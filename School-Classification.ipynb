{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import\n",
    "pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "df = pd.read_csv('/Users/zen/Downloads/dataset_with_class_labels_national_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd83eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling, select specific columns from original dataset and assign to X, select the 'Classlabel' column from original\n",
    "# dataset which contains target variable that the machine learning model will try and predict.\n",
    "X = df[['Math Proficiency %', 'English Proficiency %', 'State Assessment Proficiency Rank US', 'State Assessment Proficiency Rank AZ', 'Crime Rating', 'Graduation rate rank US ',\n",
    "'Graduation rate rank AZ']] y = df['Classlabel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and temporary set.\n",
    "X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "# Create the pipeline for SVM classifier pipeline = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('svm', SVC()) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c94ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid param_grid ={\n",
    "'svm__C': [0.1, 1, 10, 100], 'svm__kernel': ['linear', 'rbf', 'poly'], 'svm__gamma': ['scale', 'auto'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04999550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a loop to go through different values of K for K-fold cross-validation for k in range(2, 6):\n",
    "print(f\"\\nPerforming GridSearchCV with {k}-fold cross-validation.\") grid_search = GridSearchCV(pipeline, param_grid, cv=k) grid_search.fit(X_train_temp, y_train_temp) print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model on the validation set val_predictions = best_model.predict(X_val) print(f\"Validation Set Classification Report for {k}-fold CV:\") print(classification_report(y_val, val_predictions))\n",
    "# Evaluation of the model on the test set test_predictions = best_model.predict(X_test) print(f\"Test Set Classification Report for {k}-fold CV:\") print(classification_report(y_test, test_predictions))\n",
    "#SVM - Newest dataset used post data recollection, cleaning and transformation\n",
    "df = pd.read_csv('/Users/zen/Downloads/dataset_with_class_labels_national_rank_28apr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31864f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling, select specific columns from original dataset and assign to X, select the 'Classlabel' column from original\n",
    "# dataset which contains target variable that the machine learning model will try and predict.\n",
    "\n",
    "X = df[['Math Proficiency %', 'English Proficiency %', 'State Assessment Proficiency Rank US', 'State Assessment Proficiency Rank AZ', 'Crime Rating', 'Graduation rate rank US ',\n",
    "'Graduation rate rank AZ']] y = df['Classlabel']\n",
    "# Splitting the dataset into training and temporary set.\n",
    "X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the pipeline for SVM classifier pipeline = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('svm', SVC()) ])\n",
    "# Define the parameter grid param_grid ={\n",
    "'svm__C': [0.1, 1, 10, 100],\n",
    "'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "'svm__gamma': ['scale', 'auto'], # only used for rbf, poly, sigmoid\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a loop to go through different values of K for K-fold cross-validation for k in range(2, 6):\n",
    "print(f\"\\nPerforming GridSearchCV with {k}-fold cross-validation.\") grid_search = GridSearchCV(pipeline, param_grid, cv=k) grid_search.fit(X_train_temp, y_train_temp) print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "# Evaluation of the model on the validation set val_predictions = best_model.predict(X_val) print(f\"Validation Set Classification Report for {k}-fold CV:\") print(classification_report(y_val, val_predictions))\n",
    "# Evaluation of the model on the test set test_predictions = best_model.predict(X_test) print(f\"Test Set Classification Report for {k}-fold CV:\") print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e950a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN - Old dataset \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd from sklearn.model_selection \n",
    "import KFold from tensorflow.keras.models \n",
    "import Sequential from tensorflow.keras.layers \n",
    "import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f884cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/zen/Downloads/dataset_with_class_labels_national_rank.csv')\n",
    "X = df[['Math Proficiency %', 'English Proficiency %', 'State Assessment Proficiency Rank US', 'State Assessment Proficiency Rank AZ', 'Crime Rating', 'Graduation rate rank US ', 'Graduation rate rank AZ']]\n",
    "y = df.iloc[:, -1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scalingf sc = StandardScaler() X = sc.fit_transform(X)\n",
    "# Defining the K-fold Cross Validator kfold = KFold(n_splits=5, shuffle=True)\n",
    "# Initialize lists to store accuracy and loss for each fold accuracy_per_fold = []\n",
    "loss_per_fold = []\n",
    "# K-fold Cross Validation model evaluation fold_no = 1 for train, test in\n",
    "kfold.split(X, y):\n",
    "model = Sequential()\n",
    "model.add(Dense(units=6, activation='relu', input_dim=X.shape[1])) model.add(Dense(units=6, activation='relu')) model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) model.fit(X[train], y[train], epochs=100, batch_size=32, verbose=0)\n",
    "scores = model.evaluate(X[test], y[test], verbose=0) print(f'Score for fold {fold_no}: Loss = {scores[0]}, Accuracy = {scores[1]*100}%')\n",
    "accuracy_per_fold.append(scores[1] * 100) loss_per_fold.append(scores[0])\n",
    "fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ffc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of accuracy and loss across all folds print('------------------------------------------------------------------------') print('Score per fold') for i in range(len(accuracy_per_fold)): print(f'Fold {i+1}: Loss = {loss_per_fold[i]}, Accuracy = {accuracy_per_fold[i]}%') print('-------------------------- ----------------------------------------------') print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(accuracy_per_fold)}% (+- {np.std(accuracy_per_fold)})') \n",
    "print(f'> Loss: {np.mean(loss_per_fold)} (+- {np.std(loss_per_fold)})') print('---------------------------------- --------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8336686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN - New dataset \n",
    "\n",
    "df = pd.read_csv('/Users/zen/Downloads/dataset_with_class_labels_national_rank_28apr.csv')\n",
    "X = df[['Math Proficiency %', 'English Proficiency %', 'State Assessment Proficiency Rank US', 'State Assessment Proficiency Rank AZ', 'Crime Rating', 'Graduation rate rank US ', 'Graduation rate rank AZ']]\n",
    "y = df.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling sc = StandardScaler() X = sc.fit_transform(X)\n",
    "# Defining the K-fold Cross Validator kfold = KFold(n_splits=5, shuffle=True)\n",
    "# Initialize lists to store accuracy and loss for each fold accuracy_per_fold = [] loss_per_fold = []\n",
    "# K-fold Cross Validation model evaluation fold_no = 1 for train, test in kfold.split(X, y):\n",
    "model = Sequential() model.add(Dense(units=6, activation='relu', input_dim=X.shape[1])) model.add(Dense(units=6, activation='relu')) model.add(Dense(units=1, activation='sigmoid')) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) model.fit(X[train], y[train], epochs=100, batch_size=32, verbose=0)\n",
    "scores = model.evaluate(X[test], y[test], verbose=0) print(f'Score for fold {fold_no}: Loss = {scores[0]}, Accuracy = {scores[1]*100}%')\n",
    "accuracy_per_fold.append(scores[1] * 100) loss_per_fold.append(scores[0])\n",
    "fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of accuracy and loss across all folds print('------------------------------------------------------------------------') print('Score per fold') for i in range(len(accuracy_per_fold)): print(f'Fold {i+1}: Loss = {loss_per_fold[i]}, Accuracy = {accuracy_per_fold[i]}%') print('-------------------------- ----------------------------------------------') print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(accuracy_per_fold)}% (+- {np.std(accuracy_per_fold)})') print(f'> Loss: {np.mean(loss_per_fold)} (+- {np.std(loss_per_fold)})') print('---------------------------------- --------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7494b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "from sklearn.pipeline import Pipeline\n",
    "df = pd.read_csv('/Users/zen/Downloads/dataset_with_class_labels_national_rank.csv') df_cleaned = df.drop(columns=['Unnamed: 0'])\n",
    "# Separate the features and the target variable selected_columns = [ 'Math Proficiency %',\n",
    "'English Proficiency %',\n",
    "'State Assessment Proficiency Rank US', 'State Assessment Proficiency Rank AZ', 'Graduation rate rank US ',\n",
    "Tree\n",
    "'Graduation rate rank AZ',\n",
    "'Crime Rating' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned[selected_columns] y = df_cleaned['Classlabel']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Creating the Decision Tree classifier pipeline pipeline = Pipeline([\n",
    "('classifier', DecisionTreeClassifier(random_state=42)) ])\n",
    "param_grid = {\n",
    "'classifier__max_depth': [None, 10, 20, 30], 'classifier__min_samples_split': [2, 10, 20],\n",
    "'classifier__min_samples_leaf': [1, 5, 10] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38400798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to find best model parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy') grid_search.fit(X_train, y_train) best_model = grid_search.best_estimator_\n",
    "# Evaluation of best model post predicting\n",
    "predictions = best_model.predict(X_test) accuracy\n",
    "= accuracy_score(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\") print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing 5-fold cross-validation\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5) print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average CV Score: {np.mean(cv_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
